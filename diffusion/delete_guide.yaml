task:
  _target_: lock.DeleteCeleb

random_seed: 1 # 42
project_name: guideend-deletion
output_dir: XXXXXXXX/baseline/dataunlearning-BF5F/outputs
checkpoint_path: XXXXXXXX/baseline/dataunlearning-BF5F/checkpoints/google/ddpm-celebahq-256 # checkpoints/cifar/base/final/checkpoint-550000 # google/ddpm-cifar10-32 # checkpoints/cifar/base/2024-04-17_03-08-11/checkpoint-60000 #checkpoints/cifar/base/pretrained/ddpm_ema_cifar10
data_dir: XXXXXXXX/baseline/dataunlearning-BF5F/data/datasets/celeb

deletion:
  # img_name: [10000.jpg, 10001.jpg, 10002.jpg, 10003.jpg, 10004.jpg, 10005.jpg, 10006.jpg, 10007.jpg, 10008.jpg, 10009.jpg, 10010.jpg, 10011.jpg, 10012.jpg, 10013.jpg, 10014.jpg, 10015.jpg, 10016.jpg, 10017.jpg, 10018.jpg, 10019.jpg, 10020.jpg, 10021.jpg, 10022.jpg, 10023.jpg, 10024.jpg, 10025.jpg, 10026.jpg, 10027.jpg, 10028.jpg, 10029.jpg, 10030.jpg, 10031.jpg, 10032.jpg, 10033.jpg, 10034.jpg, 10035.jpg, 10036.jpg, 10037.jpg, 10038.jpg, 10039.jpg, 10040.jpg, 10041.jpg, 10042.jpg, 10043.jpg, 10044.jpg, 10045.jpg, 10046.jpg, 10047.jpg, 10048.jpg, 10049.jpg] # must be list
  # img_name: [10003.jpg, 10004.jpg]
  img_name: [27390.jpg]
  target_name: [10130.jpg]
 # img_name: [088449.jpgï¼Œ0012_01.jpg,10000.jpg, 10001.jpg, 10002.jpg, 10003.jpg, 10004.jpg, 10005.jpg, 10006.jpg, 10007.jpg, 10008.jpg, 10009.jpg, 10010.jpg, 10011.jpg, 10012.jpg, 10013.jpg, 10014.jpg, 10015.jpg, 10016.jpg, 10017.jpg, 10018.jpg, 10019.jpg, 10020.jpg, 10021.jpg, 10022.jpg, 10023.jpg, 10024.jpg, 10025.jpg, 10026.jpg, 10027.jpg, 10028.jpg, 10029.jpg, 10030.jpg, 10031.jpg, 10032.jpg, 10033.jpg, 10034.jpg, 10035.jpg, 10036.jpg, 10037.jpg, 10038.jpg, 10039.jpg, 10040.jpg, 10041.jpg, 10042.jpg, 10043.jpg, 10044.jpg, 10045.jpg, 10046.jpg, 10047.jpg, 10048.jpg, 10049.jpg]
  # frac_deletion: 0.1 # what should this be??
  loss_fn: guide # Options are [importance_sampling_with_mixture, double_forward_with_neg_del, naive_del, simple_neg_del, modified_noise_obj]
  timestep_del_window: 500 # only for modified_noise_obj (500 is good)
#  scaling_norm: 500
  eta: 1e-3
  loss_params: {}
    # superfactor: 3.0
#    lambd: 0.5
  superfactor_decay: null
  # IGNORE ALL BELOW: just placeholders for running sweep
  inception_frequency: null
  training_steps: null

transform:
  _target_: torchvision.transforms.Compose
  transforms:
    - _target_: torchvision.transforms.Resize
      size: [256, 256]
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize # Normalize to range [-1, 1]
      mean: [0.5] 
      std: [0.5] 

metrics:
  classifier_cfg: null
    # _target_: metrics.classifier.Classifier
    # # classifier:
    # #   _target_: metrics.cifar_resnet.resnet56
    # classifier:
    #   _target_: hydra.utils.get_object
    #   path: torch.hub.load
    # classifier_ckpt: null
    # classifier_args: 
    #   repo_or_dir: chenyaofo/pytorch-cifar-models
    #   model: cifar10_resnet32
    #   pretrained: true
    # transform:
    #   _target_: torchvision.transforms.Compose
    #   transforms:
    #     - _target_: torchvision.transforms.Normalize # CIFAR normalizationf from (https://github.com/chenyaofo/image-classification-codebase/blob/master/conf/cifar10.conf)
    #       mean: [0.4914, 0.4822, 0.4465]
    #       std: [0.2023, 0.1994, 0.2010]
  fraction_deletion: null
  inception_score: null
  fid: null
    # class_cfg:
    #   _target_: metrics.fid.FIDEvaluator
    #   inception_batch_size: 64
    # step_frequency: 5
    # num_imgs_to_generate: 10000
    # batch_size: 16 
  denoising_injections:
    timestep: 300
    img_path: ${data_dir}/${deletion.img_name[0]}
  likelihood: # null
#    class_cfg:
#      _target_: metrics.likelihood.LikelihoodEvaluator
#      sde:
#        _target_: metrics.song_likelihood.sde_lib.VPSDE
#    step_frequency: 3
  membership_loss: null
    # class_cfg:
    #   _target_: metrics.class_membership.MembershipLoss
    #   num_image_samples: 32 #1024
    #   num_noise_samples: 32 #128
    #   eval_batch_size: 4
    # timesteps: [200, 400, 600, 800, 900]
    # plot_params: null
    # #   save_path: 'cifar_losses.png'
    # #   time_frequency: 10 # initial graph for all membership losses
    # step_frequency: 1 # re-evaluate every gradient step

ema:
  use_ema: false # Unnecessary for deletion fine-tuning (only need to load in ema weights from training at start)

subfolders:
  unet_ema: null
  noise_scheduler: null

scheduler:
  _type: pretrained
  _target_: diffusers.DDPMScheduler
  prediction_type: epsilon

sampling_steps: 1 # Specifies number of gradient steps after which to evaluate
checkpointing_steps: 60 # Specifies number of gradient steps after which to save checkpoint
training_steps: 60
warmup_steps: 0 # for lr scheduler
eval_batch_size: 1
gradient_accumulation_steps: 1
mixed_precision: null # Choices: null (defaults to fp32), "fp16", "bf16". Bf16 requires an Nvidia Ampere GPU or newer
resume_from_checkpoint: null
train_batch_size: 4 # Makes 500 steps roughly 1 epoch
dataloader_num_workers: 0
checkpoints_total_limit: null

lr_scheduler: constant
lr_warmup_steps: 0

dataset_all:
  _target_: data.src.celeb_dataset.CelebAHQ
  filter: nondeletion
  data_path: ${data_dir}
  remove_img_names: ${deletion.img_name}

dataset_deletion:
  _target_: data.src.celeb_dataset.CelebAHQ
  filter: deletion
  data_path: ${data_dir}
  remove_img_names: ${deletion.img_name}

unet:
  _target_: diffusers.UNet2DModel

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-6 #5e-6
  betas: 
    - 0.95
    - 0.999
  weight_decay: 1e-6
  eps: 1e-8

logging:
  logger: wandb # Choices: "wandb", "tensorboard" (Wandb is significantly better supported)
  logging_dir: logs

pipeline:
  num_inference_steps: 50